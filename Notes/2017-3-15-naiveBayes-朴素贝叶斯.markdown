[博客原文链接](https://mrtriste.github.io/2017/03/15/naiveBayes-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/)

## 学习与分类

#### 数据定义

$$x_i$$为n维向量$$(x^{(1)},x^{(2)},...,x^{(n)})$$ ，表示n个特征

$$y_i$$为标记，$$y_i\in \{ c_1,c_2,...,c_m\}$$

训练集为N项数据：$$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$$

#### 目的

我们先明确一下我们的目的是什么？目的是根据一个特征向量$$(x^{(1)},x^{(2)},...,x^{(n)})$$ ，判断它属于哪个类$$y_i$$
<br>
那么也就是求$$P(y_i\mid x)$$，意思就是在输入向量为x时，判断它为$$y_i$$类的概率。然后将它是m个类的概率都求一遍，最终概率最大的就是它属于的类。

#### 求解方法

$$
P(y_i\mid x)=\frac{P(x\mid y_i)·P(y_i)}{P(x)}
$$

分子表示的乘积为x和$$y_i$$同时成立的概率，除以x成立的概率，就是在x成立的条件下，$$y_i$$成立的概率。

以上有一个隐式的条件，就是x的各个特征是独立的，也就是以下等式满足

$$
P(x\mid y_i)=P(X^{(1)=x^{(1)}},X^{(2)=x^{(3)}},...,X^{(n)=x^{(n)}}\mid y_i)\\
=\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}\mid y_i)
$$

$$P(x)$$表示x成立的概率，也就是$$P(x)=\sum_{i}{P(x{\mid y_i})P(y_i)}$$，可以看出$$P(x)$$对每个$$y_i$$都相同，所以要求$$P(y_i\mid x)$$最大，就只需求$$P(x\mid y_i)·P(y_i)$$最大，下面式子带入其中可得，

$$
P(x\mid y_i)·P(y_i)=P(y_i)·\prod_{j=1}^{n}{P(X^{(j)}=x^{(j)}\mid y_i)}
$$

我们来理解一下等式右边所表示的意思，在$$y_i​$$这种情况下，n种特征的概率相乘得到的一个概率，什么意思呢？

我们要理解什么是特征，在一个类里面，我的某个特征可以有很多个选项，它可以是具体的值，也可以是一个范围
$$X^{(j)}=x^{(j)}$$就是表示“第j个特征满足$$y_i$$这个类的第j个特征” 。那么$$P(X^{(j)}=x^{(j)}\mid y_i)$$就是表示在$$y_i$$这个类里，```第j个特征=输入的特征向量里的第j维度的特征```的比例有多少。(因为在统计学里，比例可以认为就是概率)

#### 例子

比如做一个判断QQ用户里常用账户有哪些的事情

标记有{0,1}，(1代表真实用户)

特征我们选3个，分别为$$x_1$$.日志数量/注册天数 ，$$x_2$$.好友数量/注册天数，$$x_3$$.是否使用真实头像。

值域分别为$$x_1$$：{a<=0.05, 0.05\<a\<0.2,a>=0.2} ，$$x_2$$：{a<=0.1, 0.1\<a\<0.8, a>=0.8}， $$x_3$$：{a=0,a=1}

比如我现在数据库里有100个数据，有80人是真实用户，里面$$x_2$$特征即“好友数量/注册天数”>=0.8的有50个，且我们现在确定50个人里面有48个人是真实用户,给定输入向量，x=(a>=0.2,a>=0.8,a=0)

求$$P(y=1)·P(X^{(2)}=x^{(2)}{\mid y=1})$$

解为：$$\frac{80}{100}·\frac{48}{80}$$，其实很容易理解，y=1的概率为0.8，真实用户里特征$$x_2$$为a>=0.8占48/80



## 参数估计

#### 极大似然估计

极大似然估计的先验概率：

$$
P(Y=c_k)=\frac{\sum_{i=1}^{N}{I(y_i=c_k)}}{N}\quad,k=1,2,3...,K
$$

N为所有数据的数量，分子是在所有N个数据中找到类别为$$c_k$$的数量



特征$$x^{(j)}$$的取值集合为$$\{a_{j1},a_{j2},...,a_{jS_j}\}$$

则极大似然估计的后验概率为

$$
P(X^{(j)}=a_{jl}\mid Y=c_k)=\frac{\sum_{i=1}^{N}{I(x_i^{(j)}=a_{jl},y_i=c_k)}}{\sum_{i=1}^{N}I(y_i=c_k)}
$$

j表示数据中第j个特征，$$a_{jl}$$表示特征$$x^{(j)}$$的第$$l$$个值,$$I$$函数表示有多少数量。

分子表示在$$c_k$$类别下所有特征为$$a_{jl}$$的样本数量，分母表示所有类别为$$c_k$$的样本数量。

极大似然估计其实就是以比例作概率。



以上为单个特征的后验概率，现在要将先验概率和每个特征的后验概率结合起来，计算求解方法中提到的最后一个式子

$$
P(y_i)·\prod_{j=1}^{n}{P(X^{(j)}=x^{(j)}\mid y_i)}
$$

对每个输入实例，那么也就可以计算出它属于每个类别的概率为多大，选取概率最大的类别作为他的分类。



#### 贝叶斯估计

如果按照极大似然估计计算概率，在特征没有概率为0的情况下是可以很好的工作的，但是如果遇到一个类别中的所有样本中，没有某个特征，那么用极大似然估计计算概率时会使其他特征没有起到任何作用，那么就需要其他的计算概率的办法使每个特征都能起到作用。

那就引入了贝叶斯估计，它的原理是在分子和分母上都加一个非0的数，这样即使原本分子为0，计算的概率也不会为0，且其计算的概率和依然为1，下面给出公式及简单证明：

$$
P(X^{(j)}=a_{jl}\mid Y=c_k)=\frac{\sum_{i=1}^{N}{I(x_i^{(j)}=a_{jl},y_i=c_k)}+\lambda}{\sum_{i=1}^{N}I(y_i=c_k)+S_j\lambda}
$$

第j个特征有$$S_j$$个取值，每个这种取值的概率分子上都要加一个$$\lambda$$,共加$$S_j·\lambda$$，与分母加的值相等，因此总概率依然为1。